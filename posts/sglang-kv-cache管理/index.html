<!DOCTYPE html><html lang="zh-CN"> <head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><meta name="description" content="唐晨夏的个人博客"><meta name="author" content="唐晨夏 (Chenxia Tang)"><link rel="icon" type="image/jpeg" href="/fav.jpg"><link rel="canonical" href="https://blog.tomorrowdawn.cc/posts/sglang-kv-cache%E7%AE%A1%E7%90%86"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"><!-- Open Graph --><meta property="og:title" content="SGLang KV Cache管理 | 唐晨夏的博客"><meta property="og:description" content="唐晨夏的个人博客"><meta property="og:image" content="https://blog.tomorrowdawn.cc/avatar.jpg"><meta property="og:url" content="https://blog.tomorrowdawn.cc/posts/sglang-kv-cache%E7%AE%A1%E7%90%86"><meta property="og:type" content="article"><meta property="og:site_name" content="唐晨夏的博客"><!-- Twitter Card --><meta name="twitter:card" content="summary"><meta name="twitter:title" content="SGLang KV Cache管理 | 唐晨夏的博客"><meta name="twitter:description" content="唐晨夏的个人博客"><meta name="twitter:image" content="https://blog.tomorrowdawn.cc/avatar.jpg"><!-- JSON-LD Structured Data --><script type="application/ld+json">{"@context":"https://schema.org","@graph":[{"@type":"WebSite","name":"唐晨夏的博客","url":"https://blog.tomorrowdawn.cc","author":{"@id":"https://blog.tomorrowdawn.cc/#person"}},{"@type":"Person","@id":"https://blog.tomorrowdawn.cc/#person","name":"唐晨夏","alternateName":["Chenxia Tang","TomorrowDawn"],"url":"https://blog.tomorrowdawn.cc","sameAs":["https://github.com/TomorrowDAW","https://www.zhihu.com/people/tang-chen-xia-48"]}]}</script><title>SGLang KV Cache管理 | 唐晨夏的博客</title> <script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","headline":"SGLang KV Cache管理","datePublished":"2025-12-04","author":{"@type":"Person","name":"唐晨夏","alternateName":["Chenxia Tang","TomorrowDawn"],"url":"https://blog.tomorrowdawn.cc"},"image":"https://blog.tomorrowdawn.cc/avatar.jpg","url":"https://blog.tomorrowdawn.cc/posts/sglang-kv-cache管理"}</script> <style>:root{--color-bg: #fafafa;--color-text: #333;--color-text-muted: #666;--color-border: #e0e0e0;--color-accent: #333;--color-link: #2563eb;--color-link-hover: #1d4ed8;--font-sans: system-ui, -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, sans-serif;--font-mono: "SF Mono", Monaco, "Cascadia Code", monospace;--max-width: 1200px;--sidebar-width: 280px}html{font-family:var(--font-sans);font-size:16px;line-height:1.6;color:var(--color-text);background:var(--color-bg)}body{min-height:100vh}a{color:var(--color-link);text-decoration:none;transition:color .15s ease}a:hover{color:var(--color-link-hover);text-decoration:underline}code{font-family:var(--font-mono);font-size:.9em;background:#f0f0f0;padding:.1em .3em;border-radius:3px}pre{background:#f5f5f5;padding:1rem;border-radius:4px;overflow-x:auto}pre code{background:none;padding:0}blockquote{margin:1rem 0;padding:0 1rem 0 1.5rem;position:relative;color:var(--color-text-muted)}blockquote:before{content:"";position:absolute;left:0;top:0;bottom:0;width:3px;background:var(--color-border)}.nav[data-astro-cid-dmqpwcec]{border-bottom:1px solid rgba(15,23,42,.1);background:var(--color-bg)}.nav-content[data-astro-cid-dmqpwcec]{max-width:var(--max-width);margin:0 auto;padding:1rem 2rem;display:flex;align-items:center;justify-content:space-between}.nav-links[data-astro-cid-dmqpwcec]{display:flex;gap:1.5rem}.nav-link[data-astro-cid-dmqpwcec]{color:#555;font-size:.95rem;font-weight:500;transition:color .2s ease}.nav-link[data-astro-cid-dmqpwcec]:hover{color:#111;text-decoration:none}.nav-title[data-astro-cid-dmqpwcec]{font-size:1.35rem;font-weight:700;letter-spacing:.04em;color:#111;font-family:Space Grotesk,Inter,system-ui,sans-serif}.nav-title[data-astro-cid-dmqpwcec]:hover{text-decoration:none}
.container[data-astro-cid-gysqo7gh]{max-width:800px;margin:0 auto;padding:2rem}.post-header[data-astro-cid-gysqo7gh]{margin-bottom:2rem;padding-bottom:1rem;border-bottom:1px solid var(--color-border)}.post-header[data-astro-cid-gysqo7gh] h1[data-astro-cid-gysqo7gh]{font-size:2rem;line-height:1.3;margin-bottom:1rem}.post-meta[data-astro-cid-gysqo7gh]{display:flex;align-items:center;gap:1rem;color:var(--color-text-muted);font-size:.9rem}.tags[data-astro-cid-gysqo7gh]{display:flex;gap:.5rem}.tag[data-astro-cid-gysqo7gh]{padding:.2rem .6rem;background:#f0f0f0;border-radius:3px;font-size:.8rem}.post-content[data-astro-cid-gysqo7gh]{line-height:1.8}.post-content[data-astro-cid-gysqo7gh] h2,.post-excerpt[data-astro-cid-gysqo7gh] h2{font-size:1.5rem;margin:2rem 0 1rem}.post-content[data-astro-cid-gysqo7gh] h3,.post-excerpt[data-astro-cid-gysqo7gh] h3{font-size:1.25rem;margin:1.5rem 0 .75rem}.post-content[data-astro-cid-gysqo7gh] p,.post-excerpt[data-astro-cid-gysqo7gh] p{margin-bottom:1rem}.post-content[data-astro-cid-gysqo7gh] ul,.post-content[data-astro-cid-gysqo7gh] ol,.post-excerpt[data-astro-cid-gysqo7gh] ul,.post-excerpt[data-astro-cid-gysqo7gh] ol{margin-bottom:1rem;padding-left:1.5rem}.post-content[data-astro-cid-gysqo7gh] ol,.post-excerpt[data-astro-cid-gysqo7gh] ol{list-style-type:decimal}.post-content[data-astro-cid-gysqo7gh] ul,.post-excerpt[data-astro-cid-gysqo7gh] ul{list-style-type:disc}.post-content[data-astro-cid-gysqo7gh] li,.post-excerpt[data-astro-cid-gysqo7gh] li{margin-bottom:.5rem}.post-content[data-astro-cid-gysqo7gh] blockquote,.post-excerpt[data-astro-cid-gysqo7gh] blockquote{margin:1rem 0;padding:0 1rem 0 1.5rem;position:relative;color:var(--color-text-muted)}.post-content[data-astro-cid-gysqo7gh] blockquote:before,.post-excerpt[data-astro-cid-gysqo7gh] blockquote:before{content:"";position:absolute;left:0;top:0;bottom:0;width:3px;background:var(--color-border)}.post-content[data-astro-cid-gysqo7gh] img,.post-excerpt[data-astro-cid-gysqo7gh] img{max-width:100%;height:auto;border-radius:4px}
</style></head> <body>   <nav class="nav" data-astro-cid-dmqpwcec> <div class="nav-content" data-astro-cid-dmqpwcec> <a href="/" class="nav-title" data-astro-cid-dmqpwcec>Tomorrowdawn's Blog</a> <div class="nav-links" data-astro-cid-dmqpwcec> <a href="/about" class="nav-link" data-astro-cid-dmqpwcec>About</a> </div> </div> </nav>  <main class="container" data-astro-cid-gysqo7gh> <article class="post" data-astro-cid-gysqo7gh> <header class="post-header" data-astro-cid-gysqo7gh> <h1 data-astro-cid-gysqo7gh>SGLang KV Cache管理</h1> <div class="post-meta" data-astro-cid-gysqo7gh> <time datetime="2025-12-04" data-astro-cid-gysqo7gh>2025-12-04</time> <div class="tags" data-astro-cid-gysqo7gh> <span class="tag" data-astro-cid-gysqo7gh>code</span> </div> </div> </header> <div class="post-content" data-astro-cid-gysqo7gh> <p>精心调♥教哈Gemi得到的文档，感觉可读性已经非常好了～</p>
<h2 id="sglang-kv-cache-管理深度解析">SGLang KV Cache 管理深度解析</h2>
<p>本文档深入探讨 SGLang 中 PagedAttention KV Cache 的管理机制，包括其初始化、 Block Table 的管理、Attention 计算的细节，以及一个简化的实现思路。</p>
<h3 id="1-kv-cache-的初始化物理内存池的分配">1. KV Cache 的初始化：物理内存池的分配</h3>
<p>SGLang 的 KV Cache 是一个在 GPU 上预先分配的巨大物理内存池。这个过程在每个 Python Worker 进程启动时，由 <code>sglang.srt.model_executor.model_runner.ModelRunner</code> 类通过其 <code>init_memory_pool</code> 方法完成。</p>
<h3 id="11-worker-内部架构">1.1 Worker 内部架构</h3>
<p>在深入细节之前，需要理解 Worker 进程的内部结构。当您运行 <code>python -m sglang.launch_server</code> 时，会启动一个或多个 Worker 进程。在每个 Worker 进程中：</p>
<ul>
<li><strong>主线程</strong> : 运行 <code>Scheduler</code> ( <code>sglang.srt.managers.scheduler.Scheduler</code> )，负责接收来自 Router 的请求，进行批处理（Batching），并管理逻辑块（Logical Blocks）的分配与释放。</li>
<li><strong>模型线程</strong> : 运行 <code>ModelRunner</code> ( <code>sglang.srt.model_executor.model_runner.ModelRunner</code> )，它持有一个独立的线程，专门负责在 GPU 上执行模型的前向传播。 <code>ModelRunner</code> 拥有并管理物理的 KV Cache 内存池。</li>
</ul>
<h3 id="12-物理块数量-numblocks-的计算">1.2 物理块数量 (NumBlocks) 的计算</h3>
<p><code>ModelRunner</code> 在 <code>init_memory_pool</code> 方法中调用 <code>profile_memory_for_kv_cache</code> 来确定物理块的总数 <code>NumBlocks</code> 。其计算逻辑如下：</p>
<ol>
<li><strong>获取可用 GPU 显存</strong> : 首先，通过 <code>sglang.srt.utils.get_available_gpu_memory(self.gpu_id)</code> 获取当前 GPU 的总可用显存 <code>total_gpu_memory</code> 。</li>
<li><strong>计算模型权重大小</strong> : 调用 <code>self.model.get_weight_memory_bytes()</code> 来精确计算已加载的模型权重所占用的显存 <code>weight_mem_bytes</code> 。</li>
<li><strong>计算用于 KV Cache 的总显存</strong> : SGLang 允许用户通过 <code>--mem-fraction-static</code> 参数（在 <code>ModelRunner</code> 中对应 <code>mem_fraction_static</code> ）来指定用于 KV Cache 的显存比例。计算公式为： # in sglang.srt.model_executor.model_runner.ModelRunner.profile_memory_for_kv_cache kv_cache_mem_bytes = int(total_gpu_memory * self.mem_fraction_static) - weight_mem_bytes 如果用户未指定 <code>mem_fraction_static</code> ，它默认为 <code>0.8</code> (80%)。</li>
<li><strong>计算单个物理块大小</strong> : 单个物理块所需的显存通过以下方式计算： # in sglang.srt.model_executor.model_runner.ModelRunner.profile_memory_for_kv_cache block_size = self.server_args.page_size  # 通常为 16 head_size = self.model_config.head_dim num_heads = self.model_config.get_num_kv_heads(self.tp_size) dtype_size = self.model_config.dtype_size   single_block_mem_bytes = 2 * block_size * num_heads * head_size * dtype_size 这里的 <code>2</code> 代表 Key 和 Value 两个部分。</li>
<li><strong>最终 <code>NumBlocks</code> 计算</strong> : # in sglang.srt.model_executor.model_runner.ModelRunner.profile_memory_for_kv_cache num_blocks = kv_cache_mem_bytes // single_block_mem_bytes</li>
</ol>
<h3 id="13-物理缓存的分配">1.3 物理缓存的分配</h3>
<p>在计算出 <code>num_blocks</code> 后， <code>ModelRunner</code> 在 <code>init_memory_pool</code> 方法中，通过调用 <code>torch.empty</code> 来实际分配物理缓存池。这个过程在 <code>sglang.srt.mem_cache.memory_pool.MHATokenToKVPool.__init__</code> 中完成：</p>
<pre class="astro-code github-light" style="background-color:#fff;color:#24292e; overflow-x: auto;" tabindex="0" data-language="text"><code><span class="line"><span># in sglang.srt.mem_cache.memory_pool.MHATokenToKVPool.__init__</span></span>
<span class="line"><span>self.k_cache = torch.empty(</span></span>
<span class="line"><span>    (num_blocks, self.num_heads, self.page_size, self.head_size),</span></span>
<span class="line"><span>    dtype=self.dtype,</span></span>
<span class="line"><span>    device=self.device,</span></span>
<span class="line"><span>)</span></span>
<span class="line"><span>self.v_cache = torch.empty(</span></span>
<span class="line"><span>    (num_blocks, self.num_heads, self.page_size, self.head_size),</span></span>
<span class="line"><span>    dtype=self.dtype,</span></span>
<span class="line"><span>    device=self.device,</span></span>
<span class="line"><span>)</span></span></code></pre>
<ul>
<li><strong>分布式场景</strong> : 在张量并行（TP）模式下， <code>num_heads</code> 会被替换为 <code>num_heads / tp_size</code> ，因此每个 Worker 只分配其负责的注意力头的物理缓存。</li>
</ul>
<h3 id="2-从逻辑序列到物理缓存-reqblocks-的角色">2. 从逻辑序列到物理缓存： <code>req.blocks</code> 的角色</h3>
<p>PagedAttention 的核心在于如何将一个逻辑上的 token 序列映射到物理上的、非连续的 KV 缓存块中。这个映射的桥梁，并非一个名为 <code>BlockTable</code> 的独立数据结构，而是存在于每个请求对象（ <code>Req</code> ）内部的一个关键属性： <code>blocks</code> 。</p>
<p>让我们从一个请求的视角出发，理解这个映射过程：</p>
<ol>
<li><strong>逻辑视图</strong> : 对于上层应用来说，一个请求 <code>req</code> 就是一个一维的 token 序列，例如 <code>[10, 20, 30, ...]</code> 。它的核心属性是 <code>req.input_ids</code> 和不断增长的逻辑长度 <code>req.seq_len</code> 。</li>
<li><strong>物理块映射 ( <code>req.blocks</code> )</strong> : 为了将这个逻辑序列存入 KV 缓存， <code>Scheduler</code> 会为它分配物理块。 <code>req.blocks</code> 就是一个简单的 Python <code>list</code> ，其中 <strong>按顺序</strong> 存储了分配给这个请求的 <strong>物理块的索引</strong> 。</li>
</ol>
<ul>
<li><strong>示例</strong> : 假设 <code>BlockSize=4</code> ，一个长度为 9 的请求，可能被分配了 3 个物理块，其索引分别是 <code>[7, 2, 15]</code> 。那么 <code>req.blocks</code> 的值就是 <code>[7, 2, 15]</code> 。</li>
<li>这意味着，该请求的第 0-3 个 token 的 KV 存在物理块 7 中，第 4-7 个 token 存在物理块 2 中，第 8 个 token 存在物理块 15 中。</li>
</ul>
<ol>
<li><strong>最终形态 ( <code>block_tables_tensor</code> )</strong> : 在模型进行推理之前， <code>Scheduler</code> 会将一个批次（ <code>ScheduleBatch</code> ）中所有请求的 <code>blocks</code> 列表收集起来，构建出 Attention 内核真正需要的输入：一个名为 <code>block_tables</code> 的二维 PyTorch 张量。</li>
</ol>
<ul>
<li><strong>构建过程</strong> : <code>Scheduler</code> 会找到当前批次中 <code>blocks</code> 列表最长的那个请求（假设其长度为 <code>max_blocks_per_seq</code> ），然后将所有请求的 <code>blocks</code> 列表都用0（pad）到这个长度，最后将它们堆叠（stack）成一个 <code>[batch_size, max_blocks_per_seq]</code> 的二维张量。</li>
</ul>
<p>这个从 <code>req.blocks</code> (list of lists) 到 <code>block_tables</code> (tensor) 的转换过程，清晰地展示了 SGLang 是如何将不同长度、使用不同物理块的多个请求，打包成一个统一的、可供 GPU 高效处理的批次数据的。</p>
<h3 id="21-reqblocks-的生命周期管理">2.1 <code>req.blocks</code> 的生命周期管理</h3>
<p>对 <code>req.blocks</code> 的所有操作都由 <code>sglang.srt.managers.scheduler.Scheduler</code> 类统一管理，与请求的生命周期紧密耦合。</p>
<h3 id="增-allocation"><strong>增 (Allocation)</strong></h3>
<ol>
<li><strong>时机</strong> : 当一个新请求 <code>req</code> 在 <code>Scheduler.schedule()</code> 中首次被调度执行 Prefill 时。</li>
<li><strong>调用</strong> : <code>Scheduler._alloc_request(req)</code></li>
<li><strong>过程</strong> :</li>
</ol>
<ul>
<li>根据请求的 prompt 长度 <code>req.prompt_len</code> 和 <code>BlockSize</code> ，计算出需要的物理块数量 <code>num_blocks</code> 。</li>
<li>调用 <code>self.token_to_kv_pool_allocator.alloc(num_blocks)</code> 从物理内存池中申请 <code>num_blocks</code> 个物理块。</li>
<li><code>alloc</code> 方法返回一个包含物理块索引的 Python <code>list</code> 。</li>
<li>这个 <code>list</code> 被赋值给 <code>req.blocks</code> ，完成了初始的块分配。</li>
</ul>
<h3 id="改-append"><strong>改 (Append)</strong></h3>
<ol>
<li><strong>时机</strong> : 在 Decode 阶段，当请求 <code>req</code> 生成了一个新的 token，需要为其扩展 KV Cache 时。</li>
<li><strong>调用</strong> : <code>Scheduler._append_token(req, next_token_id)</code></li>
<li><strong>过程</strong> :</li>
</ol>
<ul>
<li><code>_append_token</code> 首先检查 <code>req</code> 的最后一个逻辑块是否已满。</li>
<li>如果已满，它会调用 <code>self.token_to_kv_pool_allocator.alloc(1)</code> 申请一个新的物理块。</li>
<li>然后，将返回的新物理块索引 <code>append</code> 到 <code>req.blocks</code> 这个 list 的末尾，从而完成了 <code>req.blocks</code> 的扩展。</li>
</ul>
<h3 id="删-free"><strong>删 (Free)</strong></h3>
<ol>
<li><strong>时机</strong> : 当请求 <code>req</code> 完成（生成结束、达到最大长度或被中止）时。</li>
<li><strong>调用</strong> : <code>Scheduler._free_request(req)</code></li>
<li><strong>过程</strong> :</li>
</ol>
<ul>
<li>该方法会获取 <code>req.blocks</code> 中存储的所有物理块索引。</li>
<li>然后，它调用 <code>self.token_to_kv_pool_allocator.free(req.blocks)</code> ，将这些物理块归还到物理内存池中，以供后续请求复用。</li>
</ul>
<p>通过这种方式，SGLang 将复杂的 KV Cache 管理细节（如物理地址、碎片整理等）对上层逻辑完全屏蔽， <code>Scheduler</code> 只需要通过操作 <code>req.blocks</code> 这个简单的 Python <code>list</code> ，就能高效、灵活地管理 GPU 显存。</p>
<h3 id="3-attention-计算连接元数据与-cuda-核">3. Attention 计算：连接元数据与 CUDA 核</h3>
<p>PagedAttention 的魔法发生在底层的 CUDA Attention 核中。 <code>ModelRunner</code> 在执行模型前向传播时，会调用这些核。SGLang 支持多种 Attention 后端（如 FlashInfer , Triton ），但其核心思想是一致的：将 <code>block_table</code> 等元数据传递给核函数，使其能够处理非连续的物理内存。</p>
<h3 id="31-attention-核的调用">3.1 Attention 核的调用</h3>
<p>Attention 核的调用发生在模型（例如 <code>LlamaForCausalLM</code> ）的 <code>forward</code> 方法内部，具体是在每个 <code>LlamaAttention</code> 层中。 <code>ModelRunner</code> 会将一个 <code>ForwardBatch</code> 对象作为参数传递进去，该对象封装了所有必要的元数据。</p>
<p>一个典型的调用栈如下：</p>
<ol>
<li><code>ModelRunner.forward_extend(batch)</code></li>
<li><code>self.model.forward(..., batch=batch)</code></li>
<li><code>LlamaModel.forward(..., batch=batch)</code></li>
<li><code>LlamaAttention.forward(..., batch=batch)</code></li>
<li><code>self.attn_backend.forward(...)</code> (例如 <code>sglang.srt.layers.attention.flashinfer_backend.FlashInferBackend.forward</code> )</li>
</ol>
<p>在 <code>FlashInferBackend.forward</code> 中，最终会调用 FlashInfer 库提供的 CUDA 核，例如 <code>flashinfer.page_attention</code> 。</p>
<pre class="astro-code github-light" style="background-color:#fff;color:#24292e; overflow-x: auto;" tabindex="0" data-language="text"><code><span class="line"><span># in sglang.srt.layers.attention.flashinfer_backend.FlashInferBackend.forward_extend</span></span>
<span class="line"><span></span></span>
<span class="line"><span># ... 从 batch 对象中提取元数据 ...</span></span>
<span class="line"><span>qo_indptr = batch.qo_indptr</span></span>
<span class="line"><span>kv_indptr = batch.kv_indptr</span></span>
<span class="line"><span>kv_indices = batch.kv_indices</span></span>
<span class="line"><span>block_tables = batch.block_tables</span></span>
<span class="line"><span># ... etc</span></span>
<span class="line"><span></span></span>
<span class="line"><span>o = flashinfer.page_attention(</span></span>
<span class="line"><span>    qo_indptr,             # Query-Output Index Pointer</span></span>
<span class="line"><span>    kv_indptr,             # Key-Value Index Pointer</span></span>
<span class="line"><span>    kv_indices,            # Key-Value Indices</span></span>
<span class="line"><span>    block_tables,          # The Block Table!</span></span>
<span class="line"><span>    self.k_cache,          # 物理 K-Cache 池</span></span>
<span class="line"><span>    self.v_cache,          # 物理 V-Cache 池</span></span>
<span class="line"><span>    q,                     # 当前批次的 Query 张量</span></span>
<span class="line"><span>    # ... 其他参数</span></span>
<span class="line"><span>)</span></span></code></pre>
<h3 id="32-关键元数据">3.2 关键元数据</h3>
<p>传递给 Attention 核的元数据协同工作，使得核函数能够理解非连续的内存布局。以下是 <code>flashinfer.page_attention</code> 所需的关键参数（位于 <code>batch</code> 对象中）：</p>
<ul>
<li><strong><code>q (torch.Tensor)</code></strong> : Query 张量，形状为 <code>[num_tokens, num_qo_heads, head_size]</code> ，包含了当前批次所有请求需要计算的新 token 的 query 向量。</li>
<li><strong><code>k_cache (torch.Tensor)</code></strong> , <strong><code>v_cache (torch.Tensor)</code></strong> : 指向 GPU 上完整物理缓存池的指针，形状为 <code>[num_blocks, num_kv_heads, page_size, head_size]</code> 。</li>
<li><strong><code>block_tables (torch.Tensor)</code></strong> : 形状为 <code>[batch_size, max_blocks_per_seq]</code> 的二维整数张量。这是我们之前讨论过的核心数据结构， <code>block_tables[i, j]</code> 的值是序列 <code>i</code> 的第 <code>j</code> 个逻辑块对应的物理块索引。</li>
<li><strong><code>qo_indptr (torch.Tensor)</code></strong> : Query-Output Index Pointer，形状为 <code>[batch_size + 1]</code> 的一维整数张量。它用于在扁平化的 <code>q</code> 张量中定位每个请求的 token 范围。第 <code>i</code> 个请求的 query token 位于 <code>q[qo_indptr[i]:qo_indptr[i+1]]</code> 。</li>
<li><strong><code>kv_indptr (torch.Tensor)</code></strong> : Key-Value Index Pointer，与 <code>qo_indptr</code> 类似，但它定义了每个序列在逻辑上的总长度（包括 prompt 和已生成的 token）。 <code>kv_indptr[i+1] - kv_indptr[i]</code> 就是第 <code>i</code> 个序列的 <code>context_len</code> 。</li>
</ul>
<h3 id="33-计算流程伪代码">3.3 计算流程伪代码</h3>
<p>Attention 核内部的计算可以被概念化为一个两层嵌套的循环：</p>
<pre class="astro-code github-light" style="background-color:#fff;color:#24292e; overflow-x: auto;" tabindex="0" data-language="text"><code><span class="line"><span>// 伪代码，以单个 Query token 的计算为例</span></span>
<span class="line"><span>// thread_id 对应一个特定的 head</span></span>
<span class="line"><span></span></span>
<span class="line"><span>// 1. 确定当前 token 属于哪个序列 (seq_idx)</span></span>
<span class="line"><span>//    (通过 qo_indptr 和 token 在批次中的全局索引可以反算出)</span></span>
<span class="line"><span></span></span>
<span class="line"><span>// 2. 获取该序列的上下文长度和 block_table</span></span>
<span class="line"><span>int context_len = kv_indptr[seq_idx+1] - kv_indptr[seq_idx];</span></span>
<span class="line"><span>int* block_table_for_seq = block_tables[seq_idx];</span></span>
<span class="line"><span></span></span>
<span class="line"><span>// 3. 遍历该 token 需要 attend 到的所有历史 Key/Value</span></span>
<span class="line"><span>for (int pos = 0; pos &#x3C; context_len; ++pos) {</span></span>
<span class="line"><span>    // 4. 从 block_table 中查找物理位置</span></span>
<span class="line"><span>    int logical_block_idx = pos / PAGE_SIZE;</span></span>
<span class="line"><span>    int offset_in_block = pos % PAGE_SIZE;</span></span>
<span class="line"><span>    int physical_block_idx = block_table_for_seq[logical_block_idx];</span></span>
<span class="line"><span></span></span>
<span class="line"><span>    // 5. 从物理缓存中加载 Key 和 Value</span></span>
<span class="line"><span>    //    (地址计算: physical_block_idx * block_stride + ...)</span></span>
<span class="line"><span>    Key k = K_cache[physical_block_idx][thread_id][offset_in_block];</span></span>
<span class="line"><span>    Value v = V_cache[physical_block_idx][thread_id][offset_in_block];</span></span>
<span class="line"><span></span></span>
<span class="line"><span>    // 6. 计算 attention score 并累加</span></span>
<span class="line"><span>    score = dot_product(my_query, k);</span></span>
<span class="line"><span>    // ... softmax and accumulate value</span></span>
<span class="line"><span>}</span></span></code></pre>
<p>关键在于第 4 步和第 5 步：CUDA 核利用 <code>block_table</code> 将一个逻辑上连续的 <code>pos</code> 索引，转换为物理上非连续的 <code>physical_block_idx</code> 和 <code>offset_in_block</code> ，从而正确地从 <code>K_cache</code> 和 <code>V_cache</code> 中加载出对应的 Key 和 Value 向量。这正是 PagedAttention 的精髓所在。</p> </div> </article> </main>  </body></html> 